{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4affcd5e-2cdd-41f1-ba0e-249497ac506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc93ffa-7e5a-496d-890b-db83028ef122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a5e1d2-4774-4872-92f2-66e5cf384e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea3476-4e7c-476b-8447-7b90bfc05da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer along with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")  # Replace with your model name\n",
    "token_limit = 1024  # Token limit for the model being used (adjust based on the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5c2d5f-86d7-4e3d-92b4-6ec9a27e03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_large_csv_with_dask(filepath):\n",
    "    \"\"\"\n",
    "    Load a large CSV file using Dask for memory-efficient processing.\n",
    "    \"\"\"\n",
    "    print(\"Loading large CSV file using Dask...\")\n",
    "    ddf = dd.read_csv(filepath, assume_missing=True)\n",
    "    return ddf.compute()  # Convert Dask DataFrame to Pandas DataFrame for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae135ad6-3b93-4903-9198-9cb859fa0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Compute embeddings for 'combined_text' column in batches.\n",
    "    \"\"\"\n",
    "    print(\"Computing embeddings...\")\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Embedding Batches\"):\n",
    "        batch_texts = df['combined_text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.extend(batch_embeddings.cpu().numpy())\n",
    "    df['combined_embedding'] = embeddings\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ea35a1-e384-4fcc-9e02-0a08c3fdf8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_keyword_with_embeddings(df, keyword, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Filter rows based on semantic similarity to the keyword.\n",
    "    \"\"\"\n",
    "    print(f\"Filtering rows for keyword: {keyword}\")\n",
    "    keyword_embedding = model.encode(keyword, convert_to_tensor=True).cpu().numpy()\n",
    "    df['similarity'] = [\n",
    "        util.cos_sim(embedding, keyword_embedding).item()\n",
    "        for embedding in df['combined_embedding']\n",
    "    ]\n",
    "    return df[df['similarity'] > similarity_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfabca-9922-4a54-9bcf-e14e48183532",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ba54f-993f-4a31-81bd-e9ac52d54778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_with_model(text, max_length=130, min_length=30):\n",
    "    \"\"\"\n",
    "    Generate a summary by splitting long text into chunks.\n",
    "    \"\"\"\n",
    "    sentences = text.split(\". \")  # Split text into sentences\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "\n",
    "    # Group sentences into chunks that fit within the token limit\n",
    "    for sentence in sentences:\n",
    "        # Check if adding the sentence exceeds the token limit\n",
    "        if len(tokenizer(chunk + sentence)[\"input_ids\"]) <= token_limit:\n",
    "            chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \". \"\n",
    "\n",
    "    # Add the last chunk if it's non-empty\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    # Generate summaries for each chunk\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        summaries.append(summary[0][\"summary_text\"])\n",
    "\n",
    "    return \" \".join(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3ba08-61e1-4c36-afd8-8803a56bf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text to remove duplicates and ensure clean input.\n",
    "    \"\"\"\n",
    "    # Remove duplicate sentences\n",
    "    sentences = list(set(re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)))\n",
    "    return \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ddb67-6c79-401b-ba46-32224dded0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_category(df, keyword, max_sentences=2):\n",
    "    \"\"\"\n",
    "    Generate meaningful summaries for each category using a summarization model.\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    for category in [\"Strengths\", \"Weaknesses\", \"Opportunities\", \"Threats\"]:\n",
    "        # Filter data for the current category\n",
    "        category_rows = df[df[\"categories\"].apply(lambda x: category in x)]\n",
    "        if not category_rows.empty:\n",
    "            combined_text = \" \".join(category_rows[\"combined_text\"].tolist())\n",
    "\n",
    "            # Handle empty or invalid text\n",
    "            if not combined_text.strip():\n",
    "                summaries[category] = f\"No relevant content for {category}.\"\n",
    "                continue\n",
    "\n",
    "            # Generate summary using updated function\n",
    "            summaries[category] = generate_summary_with_model(combined_text)\n",
    "        else:\n",
    "            summaries[category] = f\"No relevant content for {category}.\"\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8319c8d-1213-4c04-9460-3d3e18518403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_relevance(df, keyword, model, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Filter rows by relevance to the keyword using semantic similarity.\n",
    "    \"\"\"\n",
    "    print(f\"Filtering rows for keyword: {keyword}\")\n",
    "    keyword_embedding = model.encode(keyword, convert_to_tensor=True)\n",
    "    relevant_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        combined_text = row['combined_text']\n",
    "        text_embedding = model.encode(combined_text, convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(text_embedding, keyword_embedding).item()\n",
    "        if similarity >= similarity_threshold:\n",
    "            relevant_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(relevant_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23af3476-0b2d-4aea-8b62-5b6c3bc08bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_category_distribution(df, output_path='static/publication_distribution.png'):\n",
    "    \"\"\"\n",
    "    Create a bar chart showing the number of articles per publisher.\n",
    "    \"\"\"\n",
    "    publication_counts = df['publication'].value_counts()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    publication_counts.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Number of Articles by Publisher')\n",
    "    plt.xlabel('Publisher')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa25d268-20e5-4793-93bf-a9c08fa91bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_summarization(filepath, keyword, similarity_threshold=0.5, max_sentences=5):\n",
    "    \"\"\"\n",
    "    Run the complete summarization pipeline for a given keyword.\n",
    "    \"\"\"\n",
    "    print(f\"Running summarization for keyword: {keyword}\")\n",
    "    \n",
    "    # Step 1: Load dataset\n",
    "    df = load_large_csv_with_dask(filepath)\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    df['content'] = df['content'].fillna('')\n",
    "    df['combined_text'] = df['title'] + \". \" + df['content']\n",
    "    df['categories'] = df['categories'].apply(eval)\n",
    "\n",
    "    # Step 2: Compute embeddings\n",
    "    df = compute_embeddings(df)\n",
    "\n",
    "    # Step 3: Filter by keyword\n",
    "    filtered_df = filter_by_keyword_with_embeddings(df, keyword, similarity_threshold=similarity_threshold)\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"No relevant rows found for the given keyword.\")\n",
    "        return {\n",
    "            \"Strengths\": [\"No relevant content.\"],\n",
    "            \"Weaknesses\": [\"No relevant content.\"],\n",
    "            \"Opportunities\": [\"No relevant content.\"],\n",
    "            \"Threats\": [\"No relevant content.\"],\n",
    "        }, pd.DataFrame()\n",
    "\n",
    "    # Step 4: Summarize by SWOT category\n",
    "    summaries = summarize_by_category(filtered_df, keyword, max_sentences=max_sentences)\n",
    "\n",
    "    # Step 5: Visualize publication distribution\n",
    "    visualize_category_distribution(filtered_df)\n",
    "\n",
    "    return summaries, filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b3494-ba77-4c89-a58e-39eec8d3b96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
